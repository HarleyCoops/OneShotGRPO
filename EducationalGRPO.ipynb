{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Educational GRPO Training Pipeline\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HarleyCoops/OneShotGRPO/blob/main/EducationalGRPO.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "##  What You'll Learn\n",
        "\n",
        "This comprehensive notebook teaches you how to train small language models using **GRPO (Generative Reinforcement Policy Optimization)** for mathematical reasoning. You'll learn:\n",
        "\n",
        "1. **Dataset Integration**: Load and preprocess GSM8K from HuggingFace\n",
        "2. **Training Environments**: Use HuggingFace RL pipeline or Prime Intellect environments\n",
        "3. **Cloud Storage**: Save checkpoints to Google Cloud Storage\n",
        "4. **Advanced Monitoring**: Track training with Weights & Biases 3D visualizations\n",
        "5. **Model Deployment**: Push to HuggingFace Hub with model cards\n",
        "6. **Interactive Inference**: Create a Gradio chat interface\n",
        "\n",
        "##  Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "- Understand GRPO and reinforcement learning for LLMs\n",
        "- Configure reward functions for math reasoning\n",
        "- Monitor training dynamics with comprehensive metrics\n",
        "- Deploy production-ready models with proper documentation\n",
        "- Build user-facing chat interfaces\n",
        "\n",
        "##  Prerequisites\n",
        "\n",
        "- Basic Python knowledge\n",
        "- Understanding of neural networks\n",
        "- Google Colab with GPU runtime (recommended: A100)\n",
        "- HuggingFace account (for model deployment)\n",
        "- Weights & Biases account (optional, for monitoring)\n",
        "- Google Cloud project (optional, for GCS checkpoints)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Section 1: Environment Setup\n",
        "\n",
        "### Understanding the Stack\n",
        "\n",
        "We'll use several specialized libraries:\n",
        "\n",
        "1. **vLLM**: High-performance inference engine with PagedAttention\n",
        "   - Reduces memory usage by 50%+\n",
        "   - Enables efficient batch processing during training\n",
        "   - Must be installed BEFORE TRL to avoid conflicts\n",
        "\n",
        "2. **TRL (Transformer Reinforcement Learning)**: RL training framework\n",
        "   - Implements GRPO, PPO, DPO algorithms\n",
        "   - Integrates with HuggingFace Transformers\n",
        "   - Handles reward computation and policy updates\n",
        "\n",
        "3. **Datasets**: Efficient data loading and processing\n",
        "   - Streaming support for large datasets\n",
        "   - Built-in caching and versioning\n",
        "   - Native HuggingFace Hub integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 1: Install vLLM (must be first!)\n",
        "print(\" Installing vLLM for efficient inference...\")\n",
        "!pip install -q vllm\n",
        "\n",
        "print(\"\\n  IMPORTANT: Restart runtime after vLLM installation!\")\n",
        "print(\"Go to Runtime > Restart runtime, then continue with the next cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 2: Install remaining dependencies\n",
        "print(\" Installing TRL, datasets, and utilities...\")\n",
        "!pip install -q trl datasets transformers wandb google-cloud-storage gradio huggingface_hub\n",
        "\n",
        "print(\"\\n All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 3: Import libraries and verify installation\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "import wandb\n",
        "\n",
        "print(\"\\n Environment Check:\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"  Warning: No GPU detected. Training will be very slow!\")\n",
        "\n",
        "print(\"\\n All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##  Section 2: Weights & Biases Setup\n",
        "\n",
        "### Why W&B for GRPO?\n",
        "\n",
        "Weights & Biases provides:\n",
        "- **Real-time metrics**: Track loss, rewards, KL divergence\n",
        "- **3D visualizations**: Plot reward landscapes and policy evolution\n",
        "- **Hyperparameter tracking**: Compare runs automatically\n",
        "- **Artifact versioning**: Track datasets and model checkpoints\n",
        "\n",
        "### Advanced Logging Strategy\n",
        "\n",
        "We'll log:\n",
        "1. Training metrics (loss, learning rate, grad norm)\n",
        "2. Reward signals (correctness, format, overall)\n",
        "3. Generation samples (input prompts + model outputs)\n",
        "4. 3D reward landscapes (reward vs. step vs. example)\n",
        "5. Model architecture and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Weights & Biases\n",
        "print(\" W&B Authentication\")\n",
        "wandb.login()\n",
        "\n",
        "# Configuration for this run\n",
        "WANDB_PROJECT = \"grpo-math-education\"\n",
        "WANDB_ENTITY = None  # Will use your default entity\n",
        "RUN_NAME = f\"grpo-qwen-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        "\n",
        "print(f\"\\n W&B Project: {WANDB_PROJECT}\")\n",
        "print(f\" Run Name: {RUN_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##  Section 3: Google Cloud Storage Setup (Optional)\n",
        "\n",
        "### Why Use GCS for Checkpoints?\n",
        "\n",
        "Google Cloud Storage advantages:\n",
        "- **Persistent storage**: Survives Colab session disconnects\n",
        "- **Large capacity**: No 15GB Drive limit\n",
        "- **Fast access**: Better upload/download speeds\n",
        "- **Versioning**: Built-in checkpoint history\n",
        "- **Team sharing**: Easy collaboration\n",
        "\n",
        "### Alternative: Google Drive\n",
        "\n",
        "If you don't have GCS, we'll use Google Drive (simpler but slower)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose your storage backend\n",
        "USE_GCS = False  # Set to True if you have Google Cloud Storage\n",
        "USE_GDRIVE = True  # Set to True to use Google Drive\n",
        "\n",
        "if USE_GCS:\n",
        "    print(\"  Setting up Google Cloud Storage...\")\n",
        "    from google.colab import auth\n",
        "    from google.cloud import storage\n",
        "    \n",
        "    # Authenticate\n",
        "    auth.authenticate_user()\n",
        "    \n",
        "    # Configuration\n",
        "    GCS_PROJECT = input(\"Enter your GCP project ID: \")\n",
        "    GCS_BUCKET = input(\"Enter your GCS bucket name: \")\n",
        "    GCS_PREFIX = f\"grpo-checkpoints/{RUN_NAME}\"\n",
        "    \n",
        "    # Initialize client\n",
        "    storage_client = storage.Client(project=GCS_PROJECT)\n",
        "    bucket = storage_client.bucket(GCS_BUCKET)\n",
        "    \n",
        "    print(f\"\\n Connected to gs://{GCS_BUCKET}/{GCS_PREFIX}\")\n",
        "    \n",
        "elif USE_GDRIVE:\n",
        "    print(\" Mounting Google Drive...\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # Create checkpoint directory\n",
        "    GDRIVE_PATH = f\"/content/drive/MyDrive/grpo_checkpoints/{RUN_NAME}\"\n",
        "    os.makedirs(GDRIVE_PATH, exist_ok=True)\n",
        "    \n",
        "    print(f\"\\n Checkpoints will save to: {GDRIVE_PATH}\")\n",
        "else:\n",
        "    print(\" Using local storage (will be lost when runtime disconnects)\")\n",
        "    LOCAL_PATH = f\"/content/outputs/{RUN_NAME}\"\n",
        "    os.makedirs(LOCAL_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##  Section 4: Dataset Loading and Formatting\n",
        "\n",
        "### GSM8K Dataset\n",
        "\n",
        "**GSM8K** (Grade School Math 8K) contains:\n",
        "- 8,500+ grade-school math word problems\n",
        "- Natural language solutions with reasoning steps\n",
        "- Numerical answers\n",
        "- Created by OpenAI for evaluating mathematical reasoning\n",
        "\n",
        "### XML Format Strategy\n",
        "\n",
        "We train the model to output:\n",
        "```xml\n",
        "<reasoning>\n",
        "Step-by-step problem solving\n",
        "</reasoning>\n",
        "<answer>\n",
        "Numerical answer\n",
        "</answer>\n",
        "```\n",
        "\n",
        "This format:\n",
        "1. Encourages chain-of-thought reasoning\n",
        "2. Makes parsing answers easy\n",
        "3. Provides interpretability\n",
        "4. Enables partial credit rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the system prompt and format\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "print(\" Format templates defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Answer extraction functions\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts the answer from XML-formatted text.\n",
        "    \n",
        "    Example:\n",
        "        Input: \"<reasoning>steps</reasoning><answer>42</answer>\"\n",
        "        Output: \"42\"\n",
        "    \"\"\"\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Extracts the answer from GSM8K format (#### delimiter).\n",
        "    \n",
        "    Example:\n",
        "        Input: \"The answer is 42\\n#### 42\"\n",
        "        Output: \"42\"\n",
        "    \"\"\"\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "# Test the functions\n",
        "test_xml = \"<reasoning>2+2=4</reasoning><answer>4</answer>\"\n",
        "test_hash = \"The total is 4\\n#### 4\"\n",
        "\n",
        "print(f\"XML test: '{extract_xml_answer(test_xml)}'\")\n",
        "print(f\"Hash test: '{extract_hash_answer(test_hash)}'\")\n",
        "print(\"\\n Extraction functions working correctly\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and format GSM8K dataset\n",
        "def get_gsm8k_questions(split=\"train\", num_examples=None) -> Dataset:\n",
        "    \"\"\"\n",
        "    Loads and preprocesses GSM8K dataset for GRPO training.\n",
        "    \n",
        "    Args:\n",
        "        split: 'train' or 'test'\n",
        "        num_examples: Optional limit on number of examples\n",
        "    \n",
        "    Returns:\n",
        "        Dataset with formatted prompts and answers\n",
        "    \"\"\"\n",
        "    print(f\" Loading GSM8K {split} split...\")\n",
        "    data = load_dataset('openai/gsm8k', 'main')[split]\n",
        "    \n",
        "    if num_examples:\n",
        "        data = data.select(range(min(num_examples, len(data))))\n",
        "    \n",
        "    print(f\" Dataset size: {len(data)} examples\")\n",
        "    \n",
        "    # Transform to GRPO format\n",
        "    def format_example(example):\n",
        "        return {\n",
        "            'prompt': [\n",
        "                {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "                {'role': 'user', 'content': example['question']}\n",
        "            ],\n",
        "            'answer': extract_hash_answer(example['answer']),\n",
        "            'reference_solution': example['answer']  # Keep full solution for reference\n",
        "        }\n",
        "    \n",
        "    print(\" Formatting examples...\")\n",
        "    data = data.map(format_example)\n",
        "    \n",
        "    # Show a sample\n",
        "    print(\"\\n Sample Question:\")\n",
        "    sample = data[0]\n",
        "    print(f\"Q: {sample['prompt'][1]['content'][:200]}...\")\n",
        "    print(f\"A: {sample['answer']}\")\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Load training data\n",
        "# Use smaller subset for faster experimentation (remove num_examples for full dataset)\n",
        "dataset = get_gsm8k_questions(split=\"train\", num_examples=1000)\n",
        "print(\"\\n Dataset ready for training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##  Section 5: Reward Functions\n",
        "\n",
        "### Understanding GRPO Rewards\n",
        "\n",
        "GRPO uses **multiple reward signals** to shape model behavior:\n",
        "\n",
        "1. **Correctness Reward** (2.0 points)\n",
        "   - Primary learning signal\n",
        "   - Binary: correct answer = 2.0, incorrect = 0.0\n",
        "   - Drives mathematical accuracy\n",
        "\n",
        "2. **Format Rewards** (1.5 points total)\n",
        "   - XML structure (0.5): Tags present and properly nested\n",
        "   - Strict format (0.5): Exact newline and spacing\n",
        "   - Soft format (0.5): Flexible whitespace\n",
        "   - Ensures consistent, parseable outputs\n",
        "\n",
        "3. **Type Reward** (0.5 points)\n",
        "   - Verifies numerical answer\n",
        "   - Prevents text-only responses\n",
        "\n",
        "### Total Reward: 4.0 points\n",
        "\n",
        "This multi-objective reward encourages:\n",
        "- Correct mathematical reasoning (50%)\n",
        "- Clean, structured output (37.5%)\n",
        "- Proper answer formatting (12.5%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reward Functions with W&B logging hooks\n",
        "\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    Primary reward: Checks if extracted answer matches ground truth.\n",
        "    Weight: 2.0 (highest priority)\n",
        "    \"\"\"\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    \n",
        "    # Log sample to W&B occasionally\n",
        "    if kwargs.get('step', 0) % 50 == 0:\n",
        "        q = prompts[0][-1]['content']\n",
        "        print('-' * 20)\n",
        "        print(f\"Question: {q[:100]}...\")\n",
        "        print(f\"Expected: {answer[0]}\")\n",
        "        print(f\"Got: {extracted_responses[0]}\")\n",
        "        print(f\"Response: {responses[0][:200]}...\")\n",
        "    \n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
        "\n",
        "def int_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    Type reward: Ensures answer is numeric.\n",
        "    Weight: 0.5\n",
        "    \"\"\"\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [0.5 if r.replace('-', '').replace('.', '').isdigit() else 0.0 \n",
        "            for r in extracted_responses]\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    Strict format reward: Exact XML structure with newlines.\n",
        "    Weight: 0.5\n",
        "    \"\"\"\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r, re.DOTALL) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    Soft format reward: Flexible XML structure.\n",
        "    Weight: 0.5\n",
        "    \"\"\"\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.search(pattern, r, re.DOTALL) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def count_xml(text: str) -> float:\n",
        "    \"\"\"\n",
        "    Granular XML component scoring.\n",
        "    Awards 0.125 for each correct tag, with penalties for extra text.\n",
        "    \"\"\"\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1]) * 0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1) * 0.001\n",
        "    return max(0.0, count)\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    XML structure reward: Detailed component scoring.\n",
        "    Weight: 0.5\n",
        "    \"\"\"\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]\n",
        "\n",
        "print(\" Reward functions defined\")\n",
        "print(\"\\n Reward Structure:\")\n",
        "print(\"  Correctness:     2.0 (50.0%)\")\n",
        "print(\"  Format (strict): 0.5 (12.5%)\")\n",
        "print(\"  Format (soft):   0.5 (12.5%)\")\n",
        "print(\"  Format (XML):    0.5 (12.5%)\")\n",
        "print(\"  Type (numeric):  0.5 (12.5%)\")\n",
        "print(\"  \" + \"-\" * 30)\n",
        "print(\"  TOTAL:           4.0 (100%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##  Section 6: Model and Training Configuration\n",
        "\n",
        "### Model Selection: Qwen2.5-0.5B-Instruct\n",
        "\n",
        "**Why this model?**\n",
        "- Small enough to train on single GPU (500M parameters)\n",
        "- Pre-trained on instruction following\n",
        "- Good mathematical reasoning baseline\n",
        "- Fast inference for RL training\n",
        "\n",
        "### GRPO Hyperparameters Explained\n",
        "\n",
        "| Parameter | Value | Reasoning |\n",
        "|-----------|-------|----------|\n",
        "| `learning_rate` | 5e-6 | Small LR prevents catastrophic forgetting |\n",
        "| `num_generations` | 16 | Multiple samples for variance reduction |\n",
        "| `max_grad_norm` | 0.1 | Gradient clipping for RL stability |\n",
        "| `num_train_epochs` | 1 | Single pass prevents overfitting |\n",
        "| `warmup_ratio` | 0.1 | Gradual LR warmup for stability |\n",
        "| `bf16` | True | Memory efficiency + numerical stability |\n",
        "\n",
        "### vLLM Configuration\n",
        "\n",
        "- `vllm_gpu_memory_utilization`: 0.3 (30% for inference, 70% for training)\n",
        "- Enables PagedAttention for efficient KV caching\n",
        "- Significantly speeds up generation during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "OUTPUT_DIR = GDRIVE_PATH if USE_GDRIVE else (f\"gs://{GCS_BUCKET}/{GCS_PREFIX}\" if USE_GCS else LOCAL_PATH)\n",
        "\n",
        "print(f\" Model: {MODEL_NAME}\")\n",
        "print(f\" Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# Initialize W&B run with comprehensive config\n",
        "wandb.init(\n",
        "    project=WANDB_PROJECT,\n",
        "    entity=WANDB_ENTITY,\n",
        "    name=RUN_NAME,\n",
        "    config={\n",
        "        # Model config\n",
        "        \"model_name\": MODEL_NAME,\n",
        "        \"model_params\": \"500M\",\n",
        "        \n",
        "        # Training config\n",
        "        \"learning_rate\": 5e-6,\n",
        "        \"adam_beta1\": 0.9,\n",
        "        \"adam_beta2\": 0.99,\n",
        "        \"weight_decay\": 0.1,\n",
        "        \"warmup_ratio\": 0.1,\n",
        "        \"lr_scheduler\": \"cosine\",\n",
        "        \n",
        "        # Batch config\n",
        "        \"per_device_batch_size\": 1,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"effective_batch_size\": 4,\n",
        "        \n",
        "        # Generation config\n",
        "        \"num_generations\": 16,\n",
        "        \"generation_batch_size\": 16,\n",
        "        \"max_prompt_length\": 256,\n",
        "        \"max_completion_length\": 200,\n",
        "        \n",
        "        # Training duration\n",
        "        \"num_train_epochs\": 1,\n",
        "        \"dataset_size\": len(dataset),\n",
        "        \n",
        "        # Regularization\n",
        "        \"max_grad_norm\": 0.1,\n",
        "        \n",
        "        # Precision\n",
        "        \"bf16\": True,\n",
        "        \n",
        "        # vLLM\n",
        "        \"use_vllm\": True,\n",
        "        \"vllm_gpu_memory\": 0.3,\n",
        "        \n",
        "        # Reward weights\n",
        "        \"reward_correctness_weight\": 2.0,\n",
        "        \"reward_format_weight\": 1.5,\n",
        "        \"reward_type_weight\": 0.5,\n",
        "    },\n",
        "    tags=[\"grpo\", \"math\", \"gsm8k\", \"educational\"]\n",
        ")\n",
        "\n",
        "print(\"\\n W&B run initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure GRPO training\n",
        "training_args = GRPOConfig(\n",
        "    # Output\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    run_name=RUN_NAME,\n",
        "    \n",
        "    # Optimizer\n",
        "    learning_rate=5e-6,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.1,\n",
        "    max_grad_norm=0.1,\n",
        "    \n",
        "    # Learning rate schedule\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type='cosine',\n",
        "    \n",
        "    # Logging\n",
        "    logging_steps=1,\n",
        "    report_to=\"wandb\",\n",
        "    log_on_each_node=False,\n",
        "    \n",
        "    # Precision\n",
        "    bf16=True,\n",
        "    \n",
        "    # Batch configuration\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    \n",
        "    # Generation parameters\n",
        "    num_generations=16,\n",
        "    generation_batch_size=16,\n",
        "    max_prompt_length=256,\n",
        "    max_completion_length=200,\n",
        "    \n",
        "    # Training duration\n",
        "    num_train_epochs=1,\n",
        "    \n",
        "    # Checkpointing\n",
        "    save_steps=100,\n",
        "    save_total_limit=5,  # Keep last 5 checkpoints\n",
        "    \n",
        "    # vLLM configuration\n",
        "    use_vllm=True,\n",
        "    vllm_gpu_memory_utilization=0.3,\n",
        ")\n",
        "\n",
        "print(\" Training configuration created\")\n",
        "print(f\"\\n Training will run for ~{len(dataset) // 4} steps\")\n",
        "print(f\" Checkpoints every 100 steps → ~{(len(dataset) // 4) // 100} checkpoints\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "print(f\" Loading model: {MODEL_NAME}...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=None\n",
        ").to(\"cuda\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Calculate model size\n",
        "param_count = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n Model Statistics:\")\n",
        "print(f\"  Total parameters: {param_count:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"  Model size: ~{param_count * 2 / 1024**3:.2f} GB (bf16)\")\n",
        "\n",
        "# Log to W&B\n",
        "wandb.config.update({\n",
        "    \"total_params\": param_count,\n",
        "    \"trainable_params\": trainable_params\n",
        "})\n",
        "\n",
        "print(\"\\n Model loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##  Section 7: Training Execution\n",
        "\n",
        "### What Happens During Training?\n",
        "\n",
        "Each training step:\n",
        "1. **Sampling**: Load batch of math problems\n",
        "2. **Generation**: Model generates 16 responses per problem\n",
        "3. **Reward**: Each response gets 5 reward scores\n",
        "4. **Policy Update**: GRPO updates model weights based on rewards\n",
        "5. **Logging**: Metrics sent to W&B\n",
        "6. **Checkpointing**: Save every 100 steps\n",
        "\n",
        "### Expected Training Time\n",
        "\n",
        "- **Dataset**: 1,000 examples\n",
        "- **Effective batch size**: 4\n",
        "- **Steps**: ~250\n",
        "- **Time per step**: ~30-60 seconds (with A100)\n",
        "- **Total time**: ~2-4 hours\n",
        "\n",
        "### Monitoring Tips\n",
        "\n",
        "Watch these metrics in W&B:\n",
        "- **Loss**: Should decrease over time\n",
        "- **Reward**: Should increase (target: 2.0+)\n",
        "- **KL Divergence**: Should stay small (<1.0)\n",
        "- **Learning Rate**: Should follow cosine schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create GRPO trainer\n",
        "print(\"  Building GRPO trainer...\")\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[\n",
        "        xmlcount_reward_func,      # 0.5 points\n",
        "        soft_format_reward_func,   # 0.5 points\n",
        "        strict_format_reward_func, # 0.5 points\n",
        "        int_reward_func,           # 0.5 points\n",
        "        correctness_reward_func,   # 2.0 points\n",
        "    ],\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "print(\"\\n Trainer ready\")\n",
        "print(\"\\n Starting training...\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Monitor your run at:\", wandb.run.get_url())\n",
        "print(\"=\"*60 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model!\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" Training complete!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nFinal metrics:\")\n",
        "print(f\"  Loss: {train_result.training_loss:.4f}\")\n",
        "print(f\"  Steps: {train_result.global_step}\")\n",
        "print(f\"  Time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
        "print(f\"  Samples/second: {train_result.metrics['train_samples_per_second']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##  Section 8: Checkpoint Management\n",
        "\n",
        "### Understanding Checkpoints\n",
        "\n",
        "Each checkpoint contains:\n",
        "- `model.safetensors`: Model weights\n",
        "- `config.json`: Model architecture\n",
        "- `tokenizer.json`: Tokenizer configuration\n",
        "- `trainer_state.json`: Training progress\n",
        "- `optimizer.pt`: Optimizer state\n",
        "- `scheduler.pt`: LR scheduler state\n",
        "\n",
        "### Selecting Best Checkpoint\n",
        "\n",
        "Strategies:\n",
        "1. **Latest**: Most training exposure\n",
        "2. **Highest reward**: Best validation performance\n",
        "3. **Lowest loss**: Most optimization progress\n",
        "\n",
        "For this demo, we'll use the **final checkpoint**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the final checkpoint\n",
        "import glob\n",
        "\n",
        "if USE_GDRIVE or not USE_GCS:\n",
        "    checkpoint_dirs = glob.glob(f\"{OUTPUT_DIR}/checkpoint-*\")\n",
        "    checkpoint_dirs.sort(key=lambda x: int(x.split(\"-\")[-1]))\n",
        "    final_checkpoint = checkpoint_dirs[-1] if checkpoint_dirs else None\n",
        "    \n",
        "    print(f\" Found {len(checkpoint_dirs)} checkpoints\")\n",
        "    if final_checkpoint:\n",
        "        print(f\" Final checkpoint: {final_checkpoint}\")\n",
        "        CHECKPOINT_PATH = final_checkpoint\n",
        "    else:\n",
        "        print(\"  No checkpoints found!\")\n",
        "        CHECKPOINT_PATH = OUTPUT_DIR\n",
        "else:\n",
        "    # For GCS, we'll need to list blobs\n",
        "    print(\" Listing GCS checkpoints...\")\n",
        "    blobs = bucket.list_blobs(prefix=GCS_PREFIX)\n",
        "    checkpoint_nums = set()\n",
        "    for blob in blobs:\n",
        "        if \"checkpoint-\" in blob.name:\n",
        "            num = blob.name.split(\"checkpoint-\")[1].split(\"/\")[0]\n",
        "            if num.isdigit():\n",
        "                checkpoint_nums.add(int(num))\n",
        "    \n",
        "    if checkpoint_nums:\n",
        "        final_num = max(checkpoint_nums)\n",
        "        CHECKPOINT_PATH = f\"gs://{GCS_BUCKET}/{GCS_PREFIX}/checkpoint-{final_num}\"\n",
        "        print(f\" Final checkpoint: {CHECKPOINT_PATH}\")\n",
        "    else:\n",
        "        print(\"  No checkpoints found in GCS!\")\n",
        "        CHECKPOINT_PATH = f\"gs://{GCS_BUCKET}/{GCS_PREFIX}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##  Section 9: Model Evaluation\n",
        "\n",
        "### Quick Inference Test\n",
        "\n",
        "Before deployment, let's test our trained model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the trained model for inference\n",
        "print(f\" Loading trained model from {CHECKPOINT_PATH}...\")\n",
        "\n",
        "inference_model = AutoModelForCausalLM.from_pretrained(\n",
        "    CHECKPOINT_PATH,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "inference_tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_PATH)\n",
        "\n",
        "print(\" Model loaded for inference\")\n",
        "\n",
        "def solve_math_problem(question: str, temperature=0.7, max_tokens=200):\n",
        "    \"\"\"\n",
        "    Generate a solution to a math problem.\n",
        "    \n",
        "    Args:\n",
        "        question: Math word problem\n",
        "        temperature: Sampling temperature (higher = more creative)\n",
        "        max_tokens: Maximum response length\n",
        "    \n",
        "    Returns:\n",
        "        dict with reasoning and answer\n",
        "    \"\"\"\n",
        "    prompt = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    \n",
        "    input_text = inference_tokenizer.apply_chat_template(\n",
        "        prompt,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    inputs = inference_tokenizer(input_text, return_tensors=\"pt\").to(inference_model.device)\n",
        "    \n",
        "    outputs = inference_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        do_sample=True,\n",
        "        pad_token_id=inference_tokenizer.pad_token_id,\n",
        "        eos_token_id=inference_tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    response = inference_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract assistant response\n",
        "    if \"assistant\" in response:\n",
        "        response = response.split(\"assistant\")[-1].strip()\n",
        "    \n",
        "    # Try to parse XML\n",
        "    try:\n",
        "        reasoning = response.split(\"<reasoning>\")[1].split(\"</reasoning>\")[0].strip()\n",
        "        answer = extract_xml_answer(response)\n",
        "    except:\n",
        "        reasoning = response\n",
        "        answer = \"Parse error\"\n",
        "    \n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": answer,\n",
        "        \"raw_response\": response\n",
        "    }\n",
        "\n",
        "# Test on sample problems\n",
        "test_problems = [\n",
        "    \"If a baker makes 24 cupcakes and puts them into boxes of 6, how many boxes does he need?\",\n",
        "    \"Sarah has 15 apples. She gives 4 to her friend and buys 8 more. How many apples does she have now?\",\n",
        "    \"A train travels 60 miles per hour. How far does it travel in 3 hours?\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" TESTING TRAINED MODEL\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "test_results = []\n",
        "for i, problem in enumerate(test_problems, 1):\n",
        "    print(f\"Test {i}/3: {problem}\")\n",
        "    result = solve_math_problem(problem)\n",
        "    test_results.append(result)\n",
        "    \n",
        "    print(f\"\\n Reasoning:\\n{result['reasoning'][:200]}...\")\n",
        "    print(f\"\\n Answer: {result['answer']}\")\n",
        "    print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
        "\n",
        "# Log test results to W&B\n",
        "wandb.log({\n",
        "    \"test_samples\": wandb.Table(\n",
        "        columns=[\"question\", \"reasoning\", \"answer\"],\n",
        "        data=[[r[\"question\"], r[\"reasoning\"], r[\"answer\"]] for r in test_results]\n",
        "    )\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##  Section 10: HuggingFace Hub Deployment\n",
        "\n",
        "### Model Card Best Practices\n",
        "\n",
        "A good model card includes:\n",
        "1. **Model Description**: What it does\n",
        "2. **Training Details**: Dataset, hyperparameters, compute\n",
        "3. **Usage Examples**: Code to run inference\n",
        "4. **Limitations**: Known issues and constraints\n",
        "5. **Evaluation Results**: Performance metrics\n",
        "6. **Citation**: How to cite your work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive model card\n",
        "MODEL_CARD = f\"\"\"\n",
        "---\n",
        "language:\n",
        "- en\n",
        "license: apache-2.0\n",
        "tags:\n",
        "- grpo\n",
        "- reinforcement-learning\n",
        "- math\n",
        "- gsm8k\n",
        "- reasoning\n",
        "base_model: {MODEL_NAME}\n",
        "datasets:\n",
        "- openai/gsm8k\n",
        "---\n",
        "\n",
        "# GRPO-Tuned Math Reasoner\n",
        "\n",
        "This model was fine-tuned using **GRPO (Generative Reinforcement Policy Optimization)** on the GSM8K dataset for mathematical reasoning tasks.\n",
        "\n",
        "## Model Description\n",
        "\n",
        "- **Base Model**: {MODEL_NAME}\n",
        "- **Training Method**: GRPO with multi-objective rewards\n",
        "- **Training Dataset**: GSM8K (Grade School Math 8K)\n",
        "- **Training Examples**: {len(dataset)}\n",
        "- **Total Parameters**: {param_count:,}\n",
        "- **Precision**: bfloat16\n",
        "\n",
        "## Training Details\n",
        "\n",
        "### Hyperparameters\n",
        "\n",
        "```yaml\n",
        "learning_rate: 5e-6\n",
        "optimizer: AdamW\n",
        "  adam_beta1: 0.9\n",
        "  adam_beta2: 0.99\n",
        "  weight_decay: 0.1\n",
        "lr_scheduler: cosine\n",
        "warmup_ratio: 0.1\n",
        "num_train_epochs: 1\n",
        "per_device_batch_size: 1\n",
        "gradient_accumulation_steps: 4\n",
        "max_grad_norm: 0.1\n",
        "num_generations: 16\n",
        "max_prompt_length: 256\n",
        "max_completion_length: 200\n",
        "```\n",
        "\n",
        "### Reward Functions\n",
        "\n",
        "The model was trained with five reward signals:\n",
        "\n",
        "1. **Correctness** (2.0 points): Exact match with ground truth\n",
        "2. **Numeric Format** (0.5 points): Answer is numeric\n",
        "3. **XML Structure** (0.5 points): Proper tag nesting\n",
        "4. **Strict Format** (0.5 points): Exact formatting\n",
        "5. **Soft Format** (0.5 points): Flexible formatting\n",
        "\n",
        "**Total possible reward**: 4.0 points\n",
        "\n",
        "### Compute Infrastructure\n",
        "\n",
        "- **GPU**: NVIDIA A100 (40GB)\n",
        "- **Training Time**: ~{train_result.metrics['train_runtime'] / 3600:.1f} hours\n",
        "- **vLLM**: Enabled for efficient inference\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"YOUR_HF_USERNAME/{RUN_NAME}\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"YOUR_HF_USERNAME/{RUN_NAME}\")\n",
        "\n",
        "# Create prompt\n",
        "messages = [\n",
        "    {{\"role\": \"system\", \"content\": \"\"\"Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\"\"\"}},\n",
        "    {{\"role\": \"user\", \"content\": \"If Sarah has 12 apples and gives away 5, how many does she have left?\"}}\n",
        "]\n",
        "\n",
        "# Generate response\n",
        "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(input_ids, max_new_tokens=200, temperature=0.7)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response)\n",
        "```\n",
        "\n",
        "### Expected Output Format\n",
        "\n",
        "```xml\n",
        "<reasoning>\n",
        "Sarah starts with 12 apples.\n",
        "She gives away 5 apples.\n",
        "To find how many she has left, we subtract: 12 - 5 = 7\n",
        "</reasoning>\n",
        "<answer>\n",
        "7\n",
        "</answer>\n",
        "```\n",
        "\n",
        "## Limitations\n",
        "\n",
        "- Trained only on grade-school level math problems\n",
        "- May struggle with complex multi-step reasoning\n",
        "- Expects numerical answers (not algebraic expressions)\n",
        "- Single epoch training may result in some underfitting\n",
        "- Performance degrades on out-of-distribution problems\n",
        "\n",
        "## Training Metrics\n",
        "\n",
        "- **Final Loss**: {train_result.training_loss:.4f}\n",
        "- **Training Steps**: {train_result.global_step}\n",
        "- **Samples/Second**: {train_result.metrics['train_samples_per_second']:.2f}\n",
        "\n",
        "## Citation\n",
        "\n",
        "```bibtex\n",
        "@misc{{{RUN_NAME.replace('-', '_')},\n",
        "  title={{GRPO-Tuned Math Reasoner}},\n",
        "  author={{Your Name}},\n",
        "  year={{2025}},\n",
        "  publisher={{HuggingFace}},\n",
        "  howpublished={{\\url{{https://huggingface.co/YOUR_USERNAME/{RUN_NAME}}}}}\n",
        "}}\n",
        "```\n",
        "\n",
        "## License\n",
        "\n",
        "This model inherits the license from the base model ({MODEL_NAME}).\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "- Base model: Qwen Team\n",
        "- Dataset: OpenAI (GSM8K)\n",
        "- Training framework: HuggingFace TRL\n",
        "- Inference engine: vLLM\n",
        "\"\"\"\n",
        "\n",
        "# Save model card\n",
        "with open(f\"{CHECKPOINT_PATH}/README.md\", \"w\") as f:\n",
        "    f.write(MODEL_CARD)\n",
        "\n",
        "print(\" Model card generated\")\n",
        "print(\"\\n Preview:\")\n",
        "print(MODEL_CARD[:500] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authenticate with HuggingFace\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "print(\" HuggingFace Authentication\")\n",
        "login()\n",
        "\n",
        "# Configure your model repository\n",
        "HF_USERNAME = input(\"Enter your HuggingFace username: \")\n",
        "HF_MODEL_NAME = input(f\"Enter model name (default: {RUN_NAME}): \") or RUN_NAME\n",
        "HF_REPO_ID = f\"{HF_USERNAME}/{HF_MODEL_NAME}\"\n",
        "\n",
        "print(f\"\\n Will push to: {HF_REPO_ID}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Push to HuggingFace Hub\n",
        "print(f\" Pushing model to {HF_REPO_ID}...\")\n",
        "\n",
        "inference_model.push_to_hub(\n",
        "    HF_REPO_ID,\n",
        "    commit_message=f\"GRPO training on GSM8K - {len(dataset)} examples\",\n",
        "    private=False  # Set to True for private repo\n",
        ")\n",
        "\n",
        "inference_tokenizer.push_to_hub(\n",
        "    HF_REPO_ID,\n",
        "    commit_message=\"Add tokenizer\"\n",
        ")\n",
        "\n",
        "print(\"\\n Model pushed successfully!\")\n",
        "print(f\"\\n View your model at: https://huggingface.co/{HF_REPO_ID}\")\n",
        "\n",
        "# Log to W&B\n",
        "wandb.config.update({\"hf_repo\": HF_REPO_ID})\n",
        "wandb.log({\"model_url\": f\"https://huggingface.co/{HF_REPO_ID}\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##  Section 11: Gradio Chat Interface\n",
        "\n",
        "### Building an Interactive Demo\n",
        "\n",
        "Let's create a simple chat interface where users can:\n",
        "1. Ask math questions\n",
        "2. See step-by-step reasoning\n",
        "3. Get the final answer\n",
        "\n",
        "This demo can be:\n",
        "- Run locally in the notebook\n",
        "- Deployed to HuggingFace Spaces\n",
        "- Embedded in websites\n",
        "- Shared via public URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create chat interface\n",
        "def chat_with_model(message, history, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Process a chat message and return the response.\n",
        "    \n",
        "    Args:\n",
        "        message: User's question\n",
        "        history: Chat history (unused in this simple version)\n",
        "        temperature: Sampling temperature\n",
        "    \n",
        "    Returns:\n",
        "        Formatted response with reasoning and answer\n",
        "    \"\"\"\n",
        "    result = solve_math_problem(message, temperature=temperature)\n",
        "    \n",
        "    # Format response nicely\n",
        "    response = f\"\"\"** Reasoning:**\n",
        "\n",
        "{result['reasoning']}\n",
        "\n",
        "** Answer:** {result['answer']}\n",
        "\"\"\"\n",
        "    \n",
        "    return response\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_with_model,\n",
        "    title=\" GRPO Math Tutor\",\n",
        "    description=f\"\"\"\n",
        "    Ask me grade-school math questions! I'll show my reasoning step-by-step.\n",
        "    \n",
        "    **Model:** {HF_REPO_ID}\n",
        "    \n",
        "    **Training:** {len(dataset)} GSM8K examples with GRPO\n",
        "    \"\"\",\n",
        "    examples=[\n",
        "        \"If a pizza is cut into 8 slices and John eats 3, what fraction is left?\",\n",
        "        \"A car travels 50 miles per hour for 2.5 hours. How far does it go?\",\n",
        "        \"Sarah has $20. She buys 3 books for $4 each. How much money does she have left?\"\n",
        "    ],\n",
        "    additional_inputs=[\n",
        "        gr.Slider(0.1, 1.5, value=0.7, label=\"Temperature (creativity)\", step=0.1)\n",
        "    ],\n",
        "    theme=gr.themes.Soft(),\n",
        "    retry_btn=\" Retry\",\n",
        "    undo_btn=\"↩ Undo\",\n",
        "    clear_btn=\" Clear\",\n",
        ")\n",
        "\n",
        "# Launch interface\n",
        "print(\" Launching Gradio interface...\")\n",
        "demo.launch(\n",
        "    share=True,  # Creates public URL\n",
        "    debug=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##  Section 12: Optional - Prime Intellect Integration\n",
        "\n",
        "### What is Prime Intellect?\n",
        "\n",
        "Prime Intellect provides:\n",
        "- **Distributed RL training**: Scale across multiple GPUs/nodes\n",
        "- **Environment Hub**: Pre-built RL environments\n",
        "- **Fault tolerance**: Automatic recovery from failures\n",
        "- **Verifiers**: Modular reward functions\n",
        "\n",
        "### When to Use Prime Intellect?\n",
        "\n",
        "Consider Prime Intellect if you need:\n",
        "- Multi-GPU/multi-node training\n",
        "- Custom RL environments\n",
        "- Production-scale deployment\n",
        "- Advanced monitoring and logging\n",
        "\n",
        "### Example: AQuA-RAT Environment\n",
        "\n",
        "The user's environment (`harleycooper/nanochatAquaRat`) is an algebra problem solver similar to GSM8K."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell demonstrates Prime Intellect integration (optional)\n",
        "# Uncomment to use\n",
        "\n",
        "\"\"\"\n",
        "# Install Prime RL\n",
        "!curl -sSL https://raw.githubusercontent.com/PrimeIntellect-ai/prime-rl/main/scripts/install.sh | bash\n",
        "\n",
        "# Configure for AQuA-RAT environment\n",
        "prime_config = {\n",
        "    \"model\": MODEL_NAME,\n",
        "    \"env\": {\n",
        "        \"id\": \"harleycooper/nanochatAquaRat\",\n",
        "        \"args\": {\n",
        "            \"num_train_examples\": 2000,\n",
        "            \"num_eval_examples\": 254,\n",
        "            \"seed\": 42\n",
        "        }\n",
        "    },\n",
        "    \"trainer\": {\n",
        "        \"args\": {\n",
        "            \"learning_rate\": 2e-5,\n",
        "            \"rollouts_per_example\": 8,\n",
        "            \"max_steps\": 400\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save config\n",
        "import toml\n",
        "with open(\"prime_config.toml\", \"w\") as f:\n",
        "    toml.dump(prime_config, f)\n",
        "\n",
        "# Run training\n",
        "!uv run vf-rl @ prime_config.toml\n",
        "\"\"\"\n",
        "\n",
        "print(\"ℹ  Prime Intellect integration code is commented out.\")\n",
        "print(\"Uncomment the cell above to use Prime Intellect environments.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##  Section 13: Wrap-Up and Next Steps\n",
        "\n",
        "### What You've Accomplished\n",
        "\n",
        " Set up a complete GRPO training pipeline\n",
        " Trained a model on mathematical reasoning\n",
        " Monitored training with Weights & Biases\n",
        " Saved checkpoints to cloud storage\n",
        " Deployed model to HuggingFace Hub\n",
        " Created an interactive chat interface\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Improve Training**:\n",
        "   - Use full GSM8K dataset (7,500 examples)\n",
        "   - Train for multiple epochs with validation\n",
        "   - Experiment with different reward weights\n",
        "   - Try larger models (1B, 7B parameters)\n",
        "\n",
        "2. **Enhance Evaluation**:\n",
        "   - Create test suite\n",
        "   - Measure accuracy on GSM8K test set\n",
        "   - Compare with baseline models\n",
        "   - Analyze failure modes\n",
        "\n",
        "3. **Deploy to Production**:\n",
        "   - Set up HuggingFace Inference Endpoint\n",
        "   - Deploy Gradio app to HF Spaces\n",
        "   - Add caching and rate limiting\n",
        "   - Monitor usage and costs\n",
        "\n",
        "4. **Extend to Other Domains**:\n",
        "   - Science QA\n",
        "   - Code generation\n",
        "   - Logical reasoning\n",
        "   - Multi-turn conversations\n",
        "\n",
        "### Resources\n",
        "\n",
        "-  [TRL Documentation](https://huggingface.co/docs/trl)\n",
        "-  [GRPO Paper](https://arxiv.org/abs/2402.03300)\n",
        "-  [vLLM Docs](https://docs.vllm.ai/)\n",
        "-  [Gradio Docs](https://gradio.app/docs)\n",
        "-   [Prime Intellect](https://primeintellect.ai/)\n",
        "\n",
        "### Questions?\n",
        "\n",
        "Check out the detailed READMEs in the repository:\n",
        "- `docs/PRIME_INTELLECT.md`\n",
        "- `docs/GOOGLE_CLOUD_STORAGE.md`\n",
        "- `docs/WANDB_VISUALIZATION.md`\n",
        "- `docs/GRADIO_DEPLOYMENT.md`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finish W&B run\n",
        "wandb.finish()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" CONGRATULATIONS! You've completed the GRPO tutorial!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\n W&B Run: {wandb.run.get_url()}\")\n",
        "print(f\" HF Model: https://huggingface.co/{HF_REPO_ID}\")\n",
        "print(f\" Checkpoints: {OUTPUT_DIR}\")\n",
        "print(\"\\n Next: Check out the docs/ folder for advanced guides!\")\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}