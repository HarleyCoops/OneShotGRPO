{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  OneShotAquaRAT - GRPO Training Pipeline\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HarleyCoops/OneShotGRPO/blob/main/OneShotAquaRAT.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "##  What You'll Learn\n",
    "\n",
    "This comprehensive notebook teaches you how to train small language models using **GRPO (Generative Reinforcement Policy Optimization)** for algebraic reasoning. You'll learn:\n",
    "\n",
    "1. **Dataset Integration**: Load and preprocess AQuA-RAT from HuggingFace\n",
    "2. **Training Environments**: Use HuggingFace RL pipeline or Prime Intellect environments\n",
    "3. **Cloud Storage**: Save checkpoints to Google Cloud Storage\n",
    "4. **Advanced Monitoring**: Track training with Weights & Biases 3D visualizations\n",
    "5. **Model Deployment**: Push to HuggingFace Hub with model cards\n",
    "6. **Interactive Inference**: Create a Gradio chat interface\n",
    "\n",
    "##  Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand GRPO and reinforcement learning for LLMs\n",
    "- Configure reward functions for algebra reasoning\n",
    "- Monitor training dynamics with comprehensive metrics\n",
    "- Deploy production-ready models with proper documentation\n",
    "- Build user-facing chat interfaces\n",
    "\n",
    "##  Prerequisites\n",
    "\n",
    "- Basic Python knowledge\n",
    "- Understanding of neural networks\n",
    "- Google Colab with GPU runtime (recommended: A100)\n",
    "- HuggingFace account (for model deployment)\n",
    "- Weights & Biases account (optional, for monitoring)\n",
    "- Google Cloud project (optional, for GCS checkpoints)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 1: Environment Setup\n",
    "\n",
    "### Understanding the Stack\n",
    "\n",
    "We'll use several specialized libraries:\n",
    "\n",
    "1. **vLLM**: High-performance inference engine with PagedAttention\n",
    "   - Reduces memory usage by 50%+\n",
    "   - Enables efficient batch processing during training\n",
    "   - Must be installed BEFORE TRL to avoid conflicts\n",
    "\n",
    "2. **TRL (Transformer Reinforcement Learning)**: RL training framework\n",
    "   - Implements GRPO, PPO, DPO algorithms\n",
    "   - Integrates with HuggingFace Transformers\n",
    "   - Handles reward computation and policy updates\n",
    "\n",
    "3. **Datasets**: Efficient data loading and processing\n",
    "   - Streaming support for large datasets\n",
    "   - Built-in caching and versioning\n",
    "   - Native HuggingFace Hub integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Install vLLM (must be first!)\n",
    "# Note: TRL requires vLLM version 0.10.2 for compatibility\n",
    "print(\" Installing vLLM 0.10.2 for efficient inference...\")\n",
    "!pip install -q vllm==0.10.2\n",
    "\n",
    "print(\"\\n  IMPORTANT: Restart runtime after vLLM installation!\")\n",
    "print(\"Go to Runtime > Restart runtime, then continue with the next cell.\")\n",
    "print(\"\\n  Note: For Colab, we'll disable vLLM server mode (use_vllm=False)\")\n",
    "print(\"        to avoid server setup complexity. Training will still work!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Install remaining dependencies\n",
    "print(\" Installing TRL, datasets, and utilities...\")\n",
    "!pip install -q trl datasets transformers wandb google-cloud-storage gradio huggingface_hub\n",
    "\n",
    "print(\"\\n All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Import libraries and verify installation\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "import wandb\n",
    "\n",
    "print(\"\\n Environment Check:\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"  Warning: No GPU detected. Training will be very slow!\")\n",
    "\n",
    "print(\"\\n All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Section 2: Weights & Biases Setup\n",
    "\n",
    "### Why W&B for GRPO?\n",
    "\n",
    "Weights & Biases provides:\n",
    "- **Real-time metrics**: Track loss, rewards, KL divergence\n",
    "- **3D visualizations**: Plot reward landscapes and policy evolution\n",
    "- **Hyperparameter tracking**: Compare runs automatically\n",
    "- **Artifact versioning**: Track datasets and model checkpoints\n",
    "\n",
    "### Advanced Logging Strategy\n",
    "\n",
    "We'll log:\n",
    "1. Training metrics (loss, learning rate, grad norm)\n",
    "2. Reward signals (correctness, format, overall)\n",
    "3. Generation samples (input prompts + model outputs)\n",
    "4. 3D reward landscapes (reward vs. step vs. example)\n",
    "5. Model architecture and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases\n",
    "print(\" W&B Authentication\")\n",
    "wandb.login()\n",
    "\n",
    "# Configuration for this run\n",
    "WANDB_PROJECT = \"grpo-algebra-education\"\n",
    "WANDB_ENTITY = None  # Will use your default entity\n",
    "RUN_NAME = f\"grpo-qwen-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "print(f\"\\n W&B Project: {WANDB_PROJECT}\")\n",
    "print(f\" Run Name: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Section 3: Google Cloud Storage Setup (Optional)\n",
    "\n",
    "### Why Use GCS for Checkpoints?\n",
    "\n",
    "Google Cloud Storage advantages:\n",
    "- **Persistent storage**: Survives Colab session disconnects\n",
    "- **Large capacity**: No 15GB Drive limit\n",
    "- **Fast access**: Better upload/download speeds\n",
    "- **Versioning**: Built-in checkpoint history\n",
    "- **Team sharing**: Easy collaboration\n",
    "\n",
    "### Alternative: Google Drive\n",
    "\n",
    "If you don't have GCS, we'll use Google Drive (simpler but slower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your storage backend\n",
    "USE_GCS = False  # Set to True if you have Google Cloud Storage\n",
    "USE_GDRIVE = True  # Set to True to use Google Drive\n",
    "\n",
    "if USE_GCS:\n",
    "    print(\"  Setting up Google Cloud Storage...\")\n",
    "    from google.colab import auth\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    # Authenticate\n",
    "    auth.authenticate_user()\n",
    "    \n",
    "    # Configuration\n",
    "    GCS_PROJECT = input(\"Enter your GCP project ID: \")\n",
    "    GCS_BUCKET = input(\"Enter your GCS bucket name: \")\n",
    "    GCS_PREFIX = f\"grpo-checkpoints/{RUN_NAME}\"\n",
    "    \n",
    "    # Initialize client\n",
    "    storage_client = storage.Client(project=GCS_PROJECT)\n",
    "    bucket = storage_client.bucket(GCS_BUCKET)\n",
    "    \n",
    "    print(f\"\\n Connected to gs://{GCS_BUCKET}/{GCS_PREFIX}\")\n",
    "    \n",
    "elif USE_GDRIVE:\n",
    "    print(\" Mounting Google Drive...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    GDRIVE_PATH = f\"/content/drive/MyDrive/grpo_checkpoints/{RUN_NAME}\"\n",
    "    os.makedirs(GDRIVE_PATH, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n Checkpoints will save to: {GDRIVE_PATH}\")\n",
    "else:\n",
    "    print(\" Using local storage (will be lost when runtime disconnects)\")\n",
    "    LOCAL_PATH = f\"/content/outputs/{RUN_NAME}\"\n",
    "    os.makedirs(LOCAL_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Dataset Loading and Formatting\n",
    "\n",
    "### AQuA-RAT Dataset\n",
    "\n",
    "**AQuA-RAT** (Algebra Question Answering with Rationales) contains:\n",
    "- ~97,000 algebra word problems\n",
    "- Multiple choice questions with 5 options (A-E)\n",
    "- Human-written rationales showing step-by-step reasoning\n",
    "- Created by DeepMind for algebraic reasoning evaluation\n",
    "\n",
    "### Multiple Choice Format Strategy\n",
    "\n",
    "We train the model to output:\n",
    "```\n",
    "[REASONING]\n",
    "Step-by-step problem solving\n",
    "[/REASONING]\n",
    "[ANSWER]\n",
    "Letter (A, B, C, D, or E)\n",
    "[/ANSWER]\n",
    "```\n",
    "\n",
    "This format:\n",
    "1. Encourages chain-of-thought reasoning\n",
    "2. Makes parsing answers easy (single letter)\n",
    "3. Provides interpretability\n",
    "4. Enables partial credit rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt and format for multiple choice\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are solving algebra problems. Respond in the following format:\n",
    "[REASONING]\n",
    "...\n",
    "[/REASONING]\n",
    "[ANSWER]\n",
    "A single letter: A, B, C, D, or E\n",
    "[/ANSWER]\n",
    "\"\"\"\n",
    "\n",
    "COT_FORMAT = \"\"\"\\\n",
    "[REASONING]\n",
    "{reasoning}\n",
    "[/REASONING]\n",
    "[ANSWER]\n",
    "{answer}\n",
    "[/ANSWER]\n",
    "\"\"\"\n",
    "\n",
    "print(\"Format templates defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer extraction functions for AQuA-RAT\n",
    "def extract_answer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the answer letter from formatted text.\n",
    "    \n",
    "    Example:\n",
    "        Input: \"[REASONING]steps[/REASONING][ANSWER]B[/ANSWER]\"\n",
    "        Output: \"B\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        answer = text.split(\"[ANSWER]\")[-1]\n",
    "        answer = answer.split(\"[/ANSWER]\")[0]\n",
    "        answer = answer.strip().upper()\n",
    "        # Return only valid letters\n",
    "        if answer in ['A', 'B', 'C', 'D', 'E']:\n",
    "            return answer\n",
    "        # Try to extract first letter\n",
    "        for char in answer:\n",
    "            if char in ['A', 'B', 'C', 'D', 'E']:\n",
    "                return char\n",
    "        return \"\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def extract_options_from_text(options_str: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse options string into dict.\n",
    "    \n",
    "    Example:\n",
    "        Input: \"A)10 B)20 C)30 D)40 E)50\"\n",
    "        Output: {'A': '10', 'B': '20', ...}\n",
    "    \"\"\"\n",
    "    options = {}\n",
    "    import re\n",
    "    # Match patterns like \"A)value\" or \"A) value\"\n",
    "    matches = re.findall(r'([A-E])\\)\\s*([^A-E)]+)', options_str)\n",
    "    for letter, value in matches:\n",
    "        options[letter] = value.strip()\n",
    "    return options\n",
    "\n",
    "# Test the functions\n",
    "test_response = \"[REASONING]Steps here[/REASONING][ANSWER]B[/ANSWER]\"\n",
    "test_options = \"A)10 B)20 C)30 D)40 E)50\"\n",
    "\n",
    "print(f\"Answer extraction test: '{extract_answer(test_response)}'\")\n",
    "print(f\"Options parsing test: {extract_options_from_text(test_options)}\")\n",
    "print(\"\\nExtraction functions working correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and format AQuA-RAT dataset\n",
    "def get_aqua_rat_questions(split=\"train\", num_examples=None) -> Dataset:\n",
    "    \"\"\"\n",
    "    Loads and preprocesses AQuA-RAT dataset for GRPO training.\n",
    "    \n",
    "    Args:\n",
    "        split: 'train', 'validation', or 'test'\n",
    "        num_examples: Optional limit on number of examples\n",
    "    \n",
    "    Returns:\n",
    "        Dataset with formatted prompts and answers\n",
    "    \"\"\"\n",
    "    print(f\"Loading AQuA-RAT {split} split...\")\n",
    "    data = load_dataset('deepmind/aqua_rat', 'raw')[split]\n",
    "    \n",
    "    if num_examples:\n",
    "        data = data.select(range(min(num_examples, len(data))))\n",
    "    \n",
    "    print(f\"Dataset size: {len(data)} examples\")\n",
    "    \n",
    "    # Transform to GRPO format\n",
    "    def format_example(example):\n",
    "        # Format question with options\n",
    "        question_text = example['question']\n",
    "        options_text = example['options']\n",
    "        \n",
    "        # Create full question\n",
    "        full_question = f\"{question_text}\\n\\nOptions:\\n{options_text}\"\n",
    "        \n",
    "        return {\n",
    "            'prompt': [\n",
    "                {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "                {'role': 'user', 'content': full_question}\n",
    "            ],\n",
    "            'answer': example['correct'],  # Single letter: A, B, C, D, or E\n",
    "            'reference_rationale': example['rationale']  # Keep rationale for reference\n",
    "        }\n",
    "    \n",
    "    print(\"Formatting examples...\")\n",
    "    data = data.map(format_example)\n",
    "    \n",
    "    # Show a sample\n",
    "    print(\"\\nSample Question:\")\n",
    "    sample = data[0]\n",
    "    print(f\"Q: {sample['prompt'][1]['content'][:150]}...\")\n",
    "    print(f\"Correct Answer: {sample['answer']}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load training data\n",
    "# Use smaller subset for faster experimentation (remove num_examples for full dataset)\n",
    "dataset = get_aqua_rat_questions(split=\"train\", num_examples=2000)\n",
    "print(\"\\nDataset ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Section 5: Reward Functions\n",
    "\n",
    "### Understanding GRPO Rewards\n",
    "\n",
    "GRPO uses **multiple reward signals** to shape model behavior:\n",
    "\n",
    "1. **Correctness Reward** (1.0 points)\n",
    "   - Primary learning signal\n",
    "   - Binary: correct letter answer = 1.0, incorrect = 0.0\n",
    "   - Drives algebraic accuracy\n",
    "\n",
    "2. **Format Rewards** (0.2 points total)\n",
    "   - Bracket structure (0.1): [REASONING] and [ANSWER] tags present\n",
    "   - Answer validation (0.1): Single letter A-E in answer section\n",
    "   - Ensures consistent, parseable outputs\n",
    "\n",
    "### Total Reward: 1.2 points\n",
    "\n",
    "This multi-objective reward encourages:\n",
    "- Correct algebraic reasoning (83.3%)\n",
    "- Clean, structured output (16.7%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward Functions with W&B logging hooks\n",
    "\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Primary reward: Checks if extracted answer matches ground truth.\n",
    "    Weight: 1.0 (highest priority)\n",
    "    \"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_answer(r) for r in responses]\n",
    "    \n",
    "    # Log sample to W&B occasionally\n",
    "    if kwargs.get('step', 0) % 50 == 0:\n",
    "        q = prompts[0][-1]['content']\n",
    "        print('-' * 20)\n",
    "        print(f\"Question: {q[:100]}...\")\n",
    "        print(f\"Expected: {answer[0]}\")\n",
    "        print(f\"Got: {extracted_responses[0]}\")\n",
    "        print(f\"Response: {responses[0][:200]}...\")\n",
    "    \n",
    "    return [1.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def letter_answer_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Answer validation reward: Ensures answer is a single valid letter (A-E).\n",
    "    Weight: 0.1\n",
    "    \"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_answer(r) for r in responses]\n",
    "    return [0.1 if r in ['A', 'B', 'C', 'D', 'E'] else 0.0 \n",
    "            for r in extracted_responses]\n",
    "\n",
    "def bracket_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Format reward: Checks for [REASONING] and [ANSWER] tags.\n",
    "    Weight: 0.1\n",
    "    \"\"\"\n",
    "    pattern = r\"\\[REASONING\\].*?\\[/REASONING\\].*?\\[ANSWER\\].*?\\[/ANSWER\\]\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.search(pattern, r, re.DOTALL) for r in responses]\n",
    "    return [0.1 if match else 0.0 for match in matches]\n",
    "\n",
    "print(\" Reward functions defined\")\n",
    "print(\"\\n Reward Structure:\")\n",
    "print(\"  Correctness:     1.0 (83.3%)\")\n",
    "print(\"  Letter format:   0.1 ( 8.3%)\")\n",
    "print(\"  Bracket tags:    0.1 ( 8.3%)\")\n",
    "print(\"  \" + \"-\" * 30)\n",
    "print(\"  TOTAL:           1.2 (100%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Section 6: Model and Training Configuration\n",
    "\n",
    "### Model Selection: Qwen2.5-0.5B-Instruct\n",
    "\n",
    "**Why this model?**\n",
    "- Small enough to train on single GPU (500M parameters)\n",
    "- Fits on Google Colab free tier GPU\n",
    "- Pre-trained on instruction following\n",
    "- Good algebraic reasoning baseline\n",
    "- Fast inference for RL training\n",
    "\n",
    "### GRPO Hyperparameters Explained\n",
    "\n",
    "| Parameter | Value | Reasoning |\n",
    "|-----------|-------|----------|\n",
    "| `learning_rate` | 5e-6 | Small LR prevents catastrophic forgetting |\n",
    "| `num_generations` | 16 | Multiple samples for variance reduction |\n",
    "| `max_grad_norm` | 0.1 | Gradient clipping for RL stability |\n",
    "| `num_train_epochs` | 1 | Single pass prevents overfitting |\n",
    "| `warmup_ratio` | 0.1 | Gradual LR warmup for stability |\n",
    "| `bf16` | True | Memory efficiency + numerical stability |\n",
    "\n",
    "### vLLM Configuration\n",
    "\n",
    "**Note**: For Colab, vLLM server mode is **disabled** (`use_vllm=False`) to avoid server setup complexity. Training will use standard transformers generation instead, which works reliably in Colab.\n",
    "\n",
    "If you want to use vLLM (for faster inference):\n",
    "- Requires vLLM version 0.10.2 (TRL compatibility)\n",
    "- Requires starting a separate vLLM server with `trl vllm-serve`\n",
    "- More complex setup but provides faster generation during training\n",
    "- For Colab, we recommend keeping `use_vllm=False` for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "OUTPUT_DIR = GDRIVE_PATH if USE_GDRIVE else (f\"gs://{GCS_BUCKET}/{GCS_PREFIX}\" if USE_GCS else LOCAL_PATH)\n",
    "\n",
    "print(f\" Model: {MODEL_NAME}\")\n",
    "print(f\" Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Initialize W&B run with comprehensive config\n",
    "wandb.init(\n",
    "    project=WANDB_PROJECT,\n",
    "    entity=WANDB_ENTITY,\n",
    "    name=RUN_NAME,\n",
    "    config={\n",
    "        # Model config\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"model_params\": \"500M\",\n",
    "        \n",
    "        # Training config\n",
    "        \"learning_rate\": 5e-6,\n",
    "        \"adam_beta1\": 0.9,\n",
    "        \"adam_beta2\": 0.99,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"lr_scheduler\": \"cosine\",\n",
    "        \n",
    "        # Batch config\n",
    "        \"per_device_batch_size\": 1,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"effective_batch_size\": 4,\n",
    "        \n",
    "        # Generation config\n",
    "        \"num_generations\": 16,\n",
    "        \"generation_batch_size\": 16,\n",
    "        \"max_prompt_length\": 256,\n",
    "        \"max_completion_length\": 200,\n",
    "        \n",
    "        # Training duration\n",
    "        \"num_train_epochs\": 1,\n",
    "        \"dataset_size\": len(dataset),\n",
    "        \n",
    "        # Regularization\n",
    "        \"max_grad_norm\": 0.1,\n",
    "        \n",
    "        # Precision\n",
    "        \"bf16\": True,\n",
    "        \n",
    "        # vLLM - DISABLED for Colab\n",
    "        \"use_vllm\": False,\n",
    "        # \"vllm_gpu_memory\": 0.3,  # Not used when use_vllm=False\n",
    "        \n",
    "        # Reward weights\n",
    "        \"reward_correctness_weight\": 1.0,\n",
    "        \"reward_letter_format_weight\": 0.1,\n",
    "        \"reward_bracket_format_weight\": 0.1,\n",
    "    },\n",
    "    tags=[\"grpo\", \"algebra\", \"aqua_rat\", \"educational\"]\n",
    ")\n",
    "\n",
    "print(\"\\n W&B run initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GRPO training\n",
    "# Note: use_vllm=False for Colab (avoids server setup complexity)\n",
    "#       Training will use standard transformers generation instead\n",
    "training_args = GRPOConfig(\n",
    "    # Output\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    run_name=RUN_NAME,\n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    max_grad_norm=0.1,\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=1,\n",
    "    report_to=\"wandb\",\n",
    "    log_on_each_node=False,\n",
    "    \n",
    "    # Precision\n",
    "    bf16=True,\n",
    "    \n",
    "    # Batch configuration\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    # Generation parameters\n",
    "    num_generations=16,\n",
    "    generation_batch_size=16,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=200,\n",
    "    \n",
    "    # Training duration\n",
    "    num_train_epochs=1,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_steps=100,\n",
    "    save_total_limit=5,  # Keep last 5 checkpoints\n",
    "    \n",
    "    # vLLM configuration - DISABLED for Colab\n",
    "    # Set to False to avoid server connection issues in Colab\n",
    "    # Training will use standard transformers generation (slightly slower but works reliably)\n",
    "    use_vllm=False,\n",
    "    # vllm_gpu_memory_utilization=0.3,  # Not needed when use_vllm=False\n",
    ")\n",
    "\n",
    "print(\" Training configuration created\")\n",
    "print(f\"\\n Training will run for ~{len(dataset) // 4} steps\")\n",
    "print(f\" Checkpoints every 100 steps → ~{(len(dataset) // 4) // 100} checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "print(f\" Loading model: {MODEL_NAME}...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=None\n",
    ").to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Calculate model size\n",
    "param_count = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n Model Statistics:\")\n",
    "print(f\"  Total parameters: {param_count:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{param_count * 2 / 1024**3:.2f} GB (bf16)\")\n",
    "\n",
    "# Log to W&B\n",
    "wandb.config.update({\n",
    "    \"total_params\": param_count,\n",
    "    \"trainable_params\": trainable_params\n",
    "})\n",
    "\n",
    "print(\"\\n Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Section 7: Training Execution\n",
    "\n",
    "### What Happens During Training?\n",
    "\n",
    "Each training step:\n",
    "1. **Sampling**: Load batch of algebra problems\n",
    "2. **Generation**: Model generates 16 responses per problem\n",
    "3. **Reward**: Each response gets 3 reward scores\n",
    "4. **Policy Update**: GRPO updates model weights based on rewards\n",
    "5. **Logging**: Metrics sent to W&B\n",
    "6. **Checkpointing**: Save every 100 steps\n",
    "\n",
    "### Expected Training Time\n",
    "\n",
    "- **Dataset**: 2,000 examples\n",
    "- **Effective batch size**: 4\n",
    "- **Steps**: ~500\n",
    "- **Time per step**: ~30-60 seconds (with A100)\n",
    "- **Total time**: ~4-8 hours\n",
    "\n",
    "### Monitoring Tips\n",
    "\n",
    "Watch these metrics in W&B:\n",
    "- **Loss**: Should decrease over time\n",
    "- **Reward**: Should increase (target: 1.0+)\n",
    "- **KL Divergence**: Should stay small (<1.0)\n",
    "- **Learning Rate**: Should follow cosine schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GRPO trainer\n",
    "print(\"  Building GRPO trainer...\")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        bracket_format_reward_func,    # 0.1 points\n",
    "        letter_answer_reward_func,     # 0.1 points\n",
    "        correctness_reward_func,       # 1.0 points\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "print(\"\\n Trainer ready\")\n",
    "print(\"\\n Starting training...\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Monitor your run at:\", wandb.run.get_url())\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model!\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" Training complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFinal metrics:\")\n",
    "print(f\"  Loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"  Steps: {train_result.global_step}\")\n",
    "print(f\"  Time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"  Samples/second: {train_result.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Section 8: Checkpoint Management\n",
    "\n",
    "### Understanding Checkpoints\n",
    "\n",
    "Each checkpoint contains:\n",
    "- `model.safetensors`: Model weights\n",
    "- `config.json`: Model architecture\n",
    "- `tokenizer.json`: Tokenizer configuration\n",
    "- `trainer_state.json`: Training progress\n",
    "- `optimizer.pt`: Optimizer state\n",
    "- `scheduler.pt`: LR scheduler state\n",
    "\n",
    "### Selecting Best Checkpoint\n",
    "\n",
    "Strategies:\n",
    "1. **Latest**: Most training exposure\n",
    "2. **Highest reward**: Best validation performance\n",
    "3. **Lowest loss**: Most optimization progress\n",
    "\n",
    "For this demo, we'll use the **final checkpoint**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the final checkpoint\n",
    "import glob\n",
    "\n",
    "if USE_GDRIVE or not USE_GCS:\n",
    "    checkpoint_dirs = glob.glob(f\"{OUTPUT_DIR}/checkpoint-*\")\n",
    "    checkpoint_dirs.sort(key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    final_checkpoint = checkpoint_dirs[-1] if checkpoint_dirs else None\n",
    "    \n",
    "    print(f\" Found {len(checkpoint_dirs)} checkpoints\")\n",
    "    if final_checkpoint:\n",
    "        print(f\" Final checkpoint: {final_checkpoint}\")\n",
    "        CHECKPOINT_PATH = final_checkpoint\n",
    "    else:\n",
    "        print(\"  No checkpoints found!\")\n",
    "        CHECKPOINT_PATH = OUTPUT_DIR\n",
    "else:\n",
    "    # For GCS, we'll need to list blobs\n",
    "    print(\" Listing GCS checkpoints...\")\n",
    "    blobs = bucket.list_blobs(prefix=GCS_PREFIX)\n",
    "    checkpoint_nums = set()\n",
    "    for blob in blobs:\n",
    "        if \"checkpoint-\" in blob.name:\n",
    "            num = blob.name.split(\"checkpoint-\")[1].split(\"/\")[0]\n",
    "            if num.isdigit():\n",
    "                checkpoint_nums.add(int(num))\n",
    "    \n",
    "    if checkpoint_nums:\n",
    "        final_num = max(checkpoint_nums)\n",
    "        CHECKPOINT_PATH = f\"gs://{GCS_BUCKET}/{GCS_PREFIX}/checkpoint-{final_num}\"\n",
    "        print(f\" Final checkpoint: {CHECKPOINT_PATH}\")\n",
    "    else:\n",
    "        print(\"  No checkpoints found in GCS!\")\n",
    "        CHECKPOINT_PATH = f\"gs://{GCS_BUCKET}/{GCS_PREFIX}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Section 9: Model Evaluation\n",
    "\n",
    "### Quick Inference Test\n",
    "\n",
    "Before deployment, let's test our trained model! \n",
    "\n",
    "**Note**: We'll load the model locally first (needed for pushing to HF Hub), then switch to using HuggingFace Inference API for all inference operations. This allows us to run inference without loading the model into GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model for inference (needed for pushing to HF Hub)\n",
    "print(f\" Loading trained model from {CHECKPOINT_PATH}...\")\n",
    "\n",
    "inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    CHECKPOINT_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "inference_tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_PATH)\n",
    "\n",
    "print(\" Model loaded for inference (local)\")\n",
    "print(\"\\n Note: After pushing to HF Hub, we'll switch to using HF Inference API\")\n",
    "print(\"       which doesn't require loading the model into GPU memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Section 10: HuggingFace Hub Deployment\n",
    "\n",
    "### Model Card Best Practices\n",
    "\n",
    "A good model card includes:\n",
    "1. **Model Description**: What it does\n",
    "2. **Training Details**: Dataset, hyperparameters, compute\n",
    "3. **Usage Examples**: Code to run inference\n",
    "4. **Limitations**: Known issues and constraints\n",
    "5. **Evaluation Results**: Performance metrics\n",
    "6. **Citation**: How to cite your work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive model card\n",
    "MODEL_CARD = f\"\"\"\n",
    "---\n",
    "language:\n",
    "- en\n",
    "license: apache-2.0\n",
    "tags:\n",
    "- grpo\n",
    "- reinforcement-learning\n",
    "- algebra\n",
    "- aqua-rat\n",
    "- reasoning\n",
    "base_model: {MODEL_NAME}\n",
    "datasets:\n",
    "- deepmind/aqua_rat\n",
    "---\n",
    "\n",
    "# GRPO-Tuned Algebra Reasoner\n",
    "\n",
    "This model was fine-tuned using **GRPO (Generative Reinforcement Policy Optimization)** on the AQuA-RAT dataset for algebraic reasoning tasks.\n",
    "\n",
    "## Model Description\n",
    "\n",
    "- **Base Model**: {MODEL_NAME}\n",
    "- **Training Method**: GRPO with multi-objective rewards\n",
    "- **Training Dataset**: AQuA-RAT (Algebra Question Answering with Rationales)\n",
    "- **Training Examples**: {len(dataset)}\n",
    "- **Total Parameters**: {param_count:,}\n",
    "- **Precision**: bfloat16\n",
    "\n",
    "## Training Details\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "```yaml\n",
    "learning_rate: 5e-6\n",
    "optimizer: AdamW\n",
    "  adam_beta1: 0.9\n",
    "  adam_beta2: 0.99\n",
    "  weight_decay: 0.1\n",
    "lr_scheduler: cosine\n",
    "warmup_ratio: 0.1\n",
    "num_train_epochs: 1\n",
    "per_device_batch_size: 1\n",
    "gradient_accumulation_steps: 4\n",
    "max_grad_norm: 0.1\n",
    "num_generations: 16\n",
    "max_prompt_length: 256\n",
    "max_completion_length: 200\n",
    "```\n",
    "\n",
    "### Reward Functions\n",
    "\n",
    "The model was trained with three reward signals:\n",
    "\n",
    "1. **Correctness** (1.0 points): Exact match with ground truth letter (A-E)\n",
    "2. **Letter Format** (0.1 points): Answer is a valid single letter\n",
    "3. **Bracket Structure** (0.1 points): Proper [REASONING]/[ANSWER] tags\n",
    "\n",
    "**Total possible reward**: 1.2 points\n",
    "\n",
    "### Compute Infrastructure\n",
    "\n",
    "- **GPU**: NVIDIA A100 (40GB)\n",
    "- **Training Time**: ~{train_result.metrics['train_runtime'] / 3600:.1f} hours\n",
    "- **vLLM**: Enabled for efficient inference\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"YOUR_HF_USERNAME/{RUN_NAME}\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"YOUR_HF_USERNAME/{RUN_NAME}\")\n",
    "\n",
    "# Create prompt\n",
    "messages = [\n",
    "    {{\"role\": \"system\", \"content\": \"\"\"You are solving algebra problems. Respond in the following format:\n",
    "[REASONING]\n",
    "...\n",
    "[/REASONING]\n",
    "[ANSWER]\n",
    "A single letter: A, B, C, D, or E\n",
    "[/ANSWER]\"\"\"}},\n",
    "    {{\"role\": \"user\", \"content\": \"If x + 5 = 12, what is x?\\\\nOptions:\\\\nA)5 B)6 C)7 D)8 E)9\"}}\n",
    "]\n",
    "\n",
    "# Generate response\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(input_ids, max_new_tokens=200, temperature=0.7)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "### Expected Output Format\n",
    "\n",
    "```\n",
    "[REASONING]\n",
    "We have the equation x + 5 = 12.\n",
    "To solve for x, subtract 5 from both sides: x = 12 - 5 = 7\n",
    "[/REASONING]\n",
    "[ANSWER]\n",
    "C\n",
    "[/ANSWER]\n",
    "```\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Trained only on algebra word problems with multiple choice answers\n",
    "- May struggle with complex multi-step reasoning\n",
    "- Expects single letter answers (A-E)\n",
    "- Single epoch training may result in some underfitting\n",
    "- Performance degrades on out-of-distribution problems\n",
    "\n",
    "## Training Metrics\n",
    "\n",
    "- **Final Loss**: {train_result.training_loss:.4f}\n",
    "- **Training Steps**: {train_result.global_step}\n",
    "- **Samples/Second**: {train_result.metrics['train_samples_per_second']:.2f}\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@misc{{{RUN_NAME.replace('-', '_')},\n",
    "  title={{GRPO-Tuned Algebra Reasoner}},\n",
    "  author={{Your Name}},\n",
    "  year={{2025}},\n",
    "  publisher={{HuggingFace}},\n",
    "  howpublished={{\\url{{https://huggingface.co/YOUR_USERNAME/{RUN_NAME}}}}}\n",
    "}}\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "This model inherits the license from the base model ({MODEL_NAME}).\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "- Base model: Qwen Team\n",
    "- Dataset: DeepMind (AQuA-RAT)\n",
    "- Training framework: HuggingFace TRL\n",
    "- Inference engine: vLLM\n",
    "\"\"\"\n",
    "\n",
    "# Save model card\n",
    "with open(f\"{CHECKPOINT_PATH}/README.md\", \"w\") as f:\n",
    "    f.write(MODEL_CARD)\n",
    "\n",
    "print(\" Model card generated\")\n",
    "print(\"\\n Preview:\")\n",
    "print(MODEL_CARD[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with HuggingFace\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "print(\" HuggingFace Authentication\")\n",
    "login()\n",
    "\n",
    "# Configure your model repository\n",
    "HF_USERNAME = input(\"Enter your HuggingFace username: \")\n",
    "HF_MODEL_NAME = input(f\"Enter model name (default: {RUN_NAME}): \") or RUN_NAME\n",
    "HF_REPO_ID = f\"{HF_USERNAME}/{HF_MODEL_NAME}\"\n",
    "\n",
    "print(f\"\\n Will push to: {HF_REPO_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to HuggingFace Hub\n",
    "print(f\" Pushing model to {HF_REPO_ID}...\")\n",
    "\n",
    "inference_model.push_to_hub(\n",
    "    HF_REPO_ID,\n",
    "    commit_message=f\"GRPO training on AQuA-RAT - {len(dataset)} examples\",\n",
    "    private=False  # Set to True for private repo\n",
    ")\n",
    "\n",
    "inference_tokenizer.push_to_hub(\n",
    "    HF_REPO_ID,\n",
    "    commit_message=\"Add tokenizer\"\n",
    ")\n",
    "\n",
    "print(\"\\n Model pushed successfully!\")\n",
    "print(f\"\\n View your model at: https://huggingface.co/{HF_REPO_ID}\")\n",
    "\n",
    "# Free up GPU memory - we'll use HF Inference API from now on\n",
    "del inference_model\n",
    "del inference_tokenizer\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n GPU memory freed - switching to HF Inference API for inference\")\n",
    "\n",
    "# Log to W&B\n",
    "wandb.config.update({\"hf_repo\": HF_REPO_ID})\n",
    "wandb.log({\"model_url\": f\"https://huggingface.co/{HF_REPO_ID}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Section 9.5: HuggingFace Inference API Setup\n",
    "\n",
    "### Why Use HF Inference API?\n",
    "\n",
    "The HuggingFace Inference API provides:\n",
    "- **No GPU memory usage**: Model runs on HF infrastructure\n",
    "- **Scalability**: Handles multiple concurrent requests\n",
    "- **Cost-effective**: Pay only for what you use\n",
    "- **Easy deployment**: No need to manage model loading/unloading\n",
    "\n",
    "### Setting Up Inference via API\n",
    "\n",
    "We'll use the `InferenceClient` from `huggingface_hub` to call our deployed model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up HuggingFace Inference API\n",
    "from huggingface_hub import InferenceClient\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\" Setting up HuggingFace Inference API...\")\n",
    "print(f\" Model: {HF_REPO_ID}\")\n",
    "\n",
    "# Initialize Inference Client\n",
    "# Note: Uses your HF token from login() automatically\n",
    "hf_client = InferenceClient(model=HF_REPO_ID)\n",
    "\n",
    "# Load tokenizer for chat template formatting (lightweight, no model weights)\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_REPO_ID)\n",
    "\n",
    "print(\"\\n HF Inference API ready!\")\n",
    "print(\" Model is running on HuggingFace infrastructure (no local GPU needed)\")\n",
    "\n",
    "def solve_algebra_problem(question: str, temperature=0.7, max_tokens=200):\n",
    "    \"\"\"\n",
    "    Generate a solution to an algebra problem using HF Inference API.\n",
    "    \n",
    "    Args:\n",
    "        question: Algebra word problem\n",
    "        temperature: Sampling temperature (higher = more creative)\n",
    "        max_tokens: Maximum response length\n",
    "    \n",
    "    Returns:\n",
    "        dict with reasoning and answer\n",
    "    \"\"\"\n",
    "    # Format prompt using chat template\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Call HF Inference API\n",
    "    try:\n",
    "        response = hf_client.text_generation(\n",
    "            formatted_prompt,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            return_full_text=False,  # Don't include the prompt in response\n",
    "            stop_sequences=[\"</s>\", \"<|endoftext|>\"]  # Stop tokens\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"reasoning\": f\"Error calling HF API: {str(e)}\",\n",
    "            \"answer\": \"Error\",\n",
    "            \"raw_response\": \"\"\n",
    "        }\n",
    "    \n",
    "    # Extract assistant response\n",
    "    if isinstance(response, str):\n",
    "        response_text = response.strip()\n",
    "    else:\n",
    "        response_text = str(response).strip()\n",
    "    \n",
    "    # Try to parse bracket format\n",
    "    try:\n",
    "        reasoning = response_text.split(\"[REASONING]\")[1].split(\"[/REASONING]\")[0].strip()\n",
    "        answer = extract_answer(response_text)\n",
    "    except:\n",
    "        reasoning = response_text\n",
    "        answer = extract_answer(response_text)\n",
    "        if not answer:\n",
    "            answer = \"Parse error\"\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"reasoning\": reasoning,\n",
    "        \"answer\": answer,\n",
    "        \"raw_response\": response_text\n",
    "    }\n",
    "\n",
    "# Test on sample problems using HF Inference API\n",
    "test_problems = [\n",
    "    \"If a store sells 5 apples for $3, how much do 15 apples cost?\\nOptions:\\nA)$6 B)$9 C)$12 D)$15 E)$18\",\n",
    "    \"A train travels at 60 mph for 2.5 hours. How far does it travel?\\nOptions:\\nA)100 miles B)120 miles C)150 miles D)180 miles E)200 miles\",\n",
    "    \"If x + 5 = 12, what is x?\\nOptions:\\nA)5 B)6 C)7 D)8 E)9\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" TESTING MODEL VIA HF INFERENCE API\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "test_results = []\n",
    "for i, problem in enumerate(test_problems, 1):\n",
    "    print(f\"Test {i}/3: {problem[:60]}...\")\n",
    "    result = solve_algebra_problem(problem)\n",
    "    test_results.append(result)\n",
    "    \n",
    "    print(f\"\\n Reasoning:\\n{result['reasoning'][:200]}...\")\n",
    "    print(f\"\\n Answer: {result['answer']}\")\n",
    "    print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "# Log test results to W&B\n",
    "wandb.log({\n",
    "    \"test_samples_hf_api\": wandb.Table(\n",
    "        columns=[\"question\", \"reasoning\", \"answer\"],\n",
    "        data=[[r[\"question\"], r[\"reasoning\"], r[\"answer\"]] for r in test_results]\n",
    "    )\n",
    "})\n",
    "\n",
    "print(\" Inference via HF API working!\")\n",
    "print(f\" Model available at: https://huggingface.co/{HF_REPO_ID}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Section 11: Gradio Chat Interface\n",
    "\n",
    "### Building an Interactive Demo\n",
    "\n",
    "Let's create a simple chat interface where users can:\n",
    "1. Ask math questions\n",
    "2. See step-by-step reasoning\n",
    "3. Get the final answer\n",
    "\n",
    "**Powered by HuggingFace Inference API**: The model runs on HF infrastructure, so you don't need a local GPU to run inference!\n",
    "\n",
    "This demo can be:\n",
    "- Run locally in the notebook (using HF API)\n",
    "- Deployed to HuggingFace Spaces\n",
    "- Embedded in websites\n",
    "- Shared via public URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Create chat interface\n",
    "def chat_with_model(message, history, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Process a chat message and return the response using HF Inference API.\n",
    "    \n",
    "    Args:\n",
    "        message: User's question\n",
    "        history: Chat history (unused in this simple version)\n",
    "        temperature: Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        Formatted response with reasoning and answer\n",
    "    \"\"\"\n",
    "    result = solve_algebra_problem(message, temperature=temperature)\n",
    "    \n",
    "    # Format response nicely\n",
    "    response = f\"\"\"** Reasoning:**\n",
    "\n",
    "{result['reasoning']}\n",
    "\n",
    "** Answer:** {result['answer']}\n",
    "\"\"\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_with_model,\n",
    "    title=\" GRPO Algebra Tutor\",\n",
    "    description=f\"\"\"\n",
    "    Ask me algebra questions! I'll show my reasoning step-by-step and provide a multiple choice answer.\n",
    "    \n",
    "    **Model:** {HF_REPO_ID}\n",
    "    \n",
    "    **Inference:** Running via HuggingFace Inference API (no local GPU needed!)\n",
    "    \n",
    "    **Training:** {len(dataset)} AQuA-RAT examples with GRPO\n",
    "    \"\"\",\n",
    "    examples=[\n",
    "        \"If x + 5 = 12, what is x?\\nOptions:\\nA)5 B)6 C)7 D)8 E)9\",\n",
    "        \"A store sells 5 apples for $3. How much do 15 apples cost?\\nOptions:\\nA)$6 B)$9 C)$12 D)$15 E)$18\",\n",
    "        \"What is 25% of 80?\\nOptions:\\nA)15 B)20 C)25 D)30 E)35\"\n",
    "    ],\n",
    "    additional_inputs=[\n",
    "        gr.Slider(0.1, 1.5, value=0.7, label=\"Temperature (creativity)\", step=0.1)\n",
    "    ],\n",
    "    theme=gr.themes.Soft(),\n",
    "    retry_btn=\" Retry\",\n",
    "    undo_btn=\"↩ Undo\",\n",
    "    clear_btn=\" Clear\",\n",
    ")\n",
    "\n",
    "# Launch interface\n",
    "print(\" Launching Gradio interface...\")\n",
    "print(f\" Using HF Inference API: {HF_REPO_ID}\")\n",
    "print(\" Model runs on HuggingFace infrastructure - no local GPU required!\")\n",
    "demo.launch(\n",
    "    share=True,  # Creates public URL\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Section 12: Optional - Prime Intellect Integration\n",
    "\n",
    "### What is Prime Intellect?\n",
    "\n",
    "Prime Intellect provides:\n",
    "- **Distributed RL training**: Scale across multiple GPUs/nodes\n",
    "- **Environment Hub**: Pre-built RL environments\n",
    "- **Fault tolerance**: Automatic recovery from failures\n",
    "- **Verifiers**: Modular reward functions\n",
    "\n",
    "### When to Use Prime Intellect?\n",
    "\n",
    "Consider Prime Intellect if you need:\n",
    "- Multi-GPU/multi-node training\n",
    "- Custom RL environments\n",
    "- Production-scale deployment\n",
    "- Advanced monitoring and logging\n",
    "\n",
    "### Example: AQuA-RAT Environment\n",
    "\n",
    "The user's environment (`harleycooper/nanochatAquaRat`) is an algebra problem solver similar to GSM8K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell demonstrates Prime Intellect integration (optional)\n",
    "# Uncomment to use\n",
    "\n",
    "\"\"\"\n",
    "# Install Prime RL\n",
    "!curl -sSL https://raw.githubusercontent.com/PrimeIntellect-ai/prime-rl/main/scripts/install.sh | bash\n",
    "\n",
    "# Configure for AQuA-RAT environment\n",
    "prime_config = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"env\": {\n",
    "        \"id\": \"harleycooper/nanochatAquaRat\",\n",
    "        \"args\": {\n",
    "            \"num_train_examples\": 2000,\n",
    "            \"num_eval_examples\": 254,\n",
    "            \"seed\": 42\n",
    "        }\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"args\": {\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"rollouts_per_example\": 8,\n",
    "            \"max_steps\": 400\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save config\n",
    "import toml\n",
    "with open(\"prime_config.toml\", \"w\") as f:\n",
    "    toml.dump(prime_config, f)\n",
    "\n",
    "# Run training\n",
    "!uv run vf-rl @ prime_config.toml\n",
    "\"\"\"\n",
    "\n",
    "print(\"ℹ  Prime Intellect integration code is commented out.\")\n",
    "print(\"Uncomment the cell above to use Prime Intellect environments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Section 13: Wrap-Up and Next Steps\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    " Set up a complete GRPO training pipeline\n",
    " Trained a model on algebraic reasoning\n",
    " Monitored training with Weights & Biases\n",
    " Saved checkpoints to cloud storage\n",
    " Deployed model to HuggingFace Hub\n",
    " Created an interactive chat interface\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Improve Training**:\n",
    "   - Use full AQuA-RAT dataset (~97,000 examples)\n",
    "   - Train for multiple epochs with validation\n",
    "   - Experiment with different reward weights\n",
    "   - Try larger models (1B, 7B parameters)\n",
    "\n",
    "2. **Enhance Evaluation**:\n",
    "   - Create test suite\n",
    "   - Measure accuracy on AQuA-RAT test set\n",
    "   - Compare with baseline models\n",
    "   - Analyze failure modes\n",
    "\n",
    "3. **Deploy to Production**:\n",
    "   - Set up HuggingFace Inference Endpoint\n",
    "   - Deploy Gradio app to HF Spaces\n",
    "   - Add caching and rate limiting\n",
    "   - Monitor usage and costs\n",
    "\n",
    "4. **Extend to Other Domains**:\n",
    "   - Math reasoning (GSM8K)\n",
    "   - Science QA\n",
    "   - Code generation\n",
    "   - Logical reasoning\n",
    "   - Multi-turn conversations\n",
    "\n",
    "### Resources\n",
    "\n",
    "-  [TRL Documentation](https://huggingface.co/docs/trl)\n",
    "-  [GRPO Paper](https://arxiv.org/abs/2402.03300)\n",
    "-  [vLLM Docs](https://docs.vllm.ai/)\n",
    "-  [Gradio Docs](https://gradio.app/docs)\n",
    "-   [Prime Intellect](https://primeintellect.ai/)\n",
    "\n",
    "### Questions?\n",
    "\n",
    "Check out the detailed READMEs in the repository:\n",
    "- `docs/PRIME_INTELLECT.md`\n",
    "- `docs/GOOGLE_CLOUD_STORAGE.md`\n",
    "- `docs/WANDB_VISUALIZATION.md`\n",
    "- `docs/GRADIO_DEPLOYMENT.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish W&B run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" CONGRATULATIONS! You've completed the GRPO tutorial!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n W&B Run: {wandb.run.get_url()}\")\n",
    "print(f\" HF Model: https://huggingface.co/{HF_REPO_ID}\")\n",
    "print(f\" Checkpoints: {OUTPUT_DIR}\")\n",
    "print(\"\\n Next: Check out the docs/ folder for advanced guides!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
